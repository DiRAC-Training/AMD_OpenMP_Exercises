{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e604041e-b2fb-446d-a8df-8385ff5cccdf",
   "metadata": {},
   "source": [
    "# Reductions, atomics and mutexes\n",
    "\n",
    "So far, we have concentrated on offloading independent, element-wise calculations over arrays to our devices.\n",
    "Although this makes up a substantial portion of the work we expect to do using our acceleration devices, we also need to understand how to access single variables from each during a distributed calculation.\n",
    "\n",
    "OpenMP provides functionality for this in the form of mutexes, reductions and atomics.\n",
    "This notebook will review this functionality, and provide some examples of them in action.\n",
    "\n",
    "Let's begin with our usual checks; making sure we have an appropriate GPU, loading into the working directory, and cleaning our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5dbdb-ca1c-415f-96ca-fb0a32d29ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rocm-smi\n",
    "cd $HOME/DiRAC-AMD-GPU/notebooks/02-OpenMP/2f-OpenMP_reductions_atomics_mutexes/C\n",
    "make clean && rm -rf build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf047cc-f8f3-4990-a1ed-f55ba4f3f578",
   "metadata": {},
   "source": [
    "## Mutexes\n",
    "\n",
    "If you've done any work with CPU-based parallelism in the past, you may already be familiar with the concept of a `mutex`.\n",
    "When multiple threads attempt to access and change a global variable with no protection, we enter what is known as a 'race condition'.\n",
    "Each thread attempts to make its own changes to this variable, potentially over-writing the work of other threads and producing innaccurate results.\n",
    "\n",
    "The traditional solution to this problem is known as a `mutex` (a portmanteau of mutual exclusion) - a lock that is placed around these potentially conflicting calculations to only allow one thread to access to the variable at a time.\n",
    "Although effective, these types of hard software locks are typically very expensive in terms of performance - a problem that is only enhanced when running with greater parallelism on a GPU or APU.\n",
    "More modern techniques techniques have been developed for the majority of coding challenges that would once have been addressed by mutexes, and so they are generally considered deprecated for device-based kernels.\n",
    "\n",
    "OpenMP does provide a number of directives that fulfil the functionality of a mutex-style lock, but it is generally recommended to try and adapt code where possible to the more modern paradigms of reductions and atomics, which we will discuss below.\n",
    "\n",
    "## Reductions\n",
    "\n",
    "A common practice that would once have been carried out using mutexes is the transformation of an array into a single variable; the calculation of the sum of the elements of said array, for example.\n",
    "If we consider running this in a single loop, it is not clear how running in parallel would speed up the calculation; you might (correctly!) assume that locking the software for each addition would slow the calculation down, and be less efficient than calculating it in series.\n",
    "What we can do to improve the performance of this calculation is to split it into a number of parallel threads that each perform their own part in series, and then summing their results.\n",
    "\n",
    "In OpenMP, these calculations are called a `reduction`, because they are used to reduce an array into a lower-dimension output.\n",
    "In order to use them, we add in the `reduction(reduction-identifier: var-list)` clause to our parallel operation.\n",
    "In this clause, the mandatory `reduction-identifier` identifier tells the compiler what operation will be used with the reduced variable, and takes the form of an `id-expression` or one of the operators: `+`, `-`, `*`, `&`, `|`, `^`, `&&` and `||`.\n",
    "`var-list` then tells the compiler which variables will be part of the calculation.\n",
    "\n",
    "When this clause is used, OpenMP creates a number of threads to divide the work between, each with a copy of the reduction inialisaed to a value determined by the reduction identifier type.\n",
    "The operation is carried out, and once all parallel regions have finished the individual variables are combined into the final reduced output.\n",
    "\n",
    "[`reduction_scalar.c`](./C/reduction_scalar.c) shows an example of a reduction clause.\n",
    "The code initialises two variables, then creates a parallel loop to increment these variables over a range.\n",
    "For this example we use `reduction(+:ce1,ce2)`, indicating that we are reducing the variables `ce1` and `ce2` with an addition operation.\n",
    "\n",
    "Let's compile and run it now, and check the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8590cf-fe9f-4a7d-a27d-1e838737dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "make reduction_scalar\n",
    "./reduction_scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a68b698-e7aa-4042-ad9c-dd3eb7f4e432",
   "metadata": {},
   "source": [
    "Reducing to a scalar value in this way is very useful and has many real-world applications.\n",
    "There are also times, however, that we might want to take an array with many dimensions and, operating over a number of these dimensions, produce an output with a reduced dimensionality.\n",
    "This is fully supported in the OpenMP standard.\n",
    "\n",
    "Indeed, in its precise definition in OpenMP, reduction is an operationg that takes an N-dimensional array and produces a lower order dimensional array output.\n",
    "This lower dimension is commonly a single scalar, as seen in the example above, but it is perfectly valid to produce any array with a lower dimensionality than the input.\n",
    "\n",
    "[`reduction_array.c`](./C/reduction_array.c) demonstrates an example of reduction to a single dimension array.\n",
    "Let's compare the reduction pragmas between this and the [`reduction_scalar.c`](./C/reduction_scalar.c) example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b72491-e8d0-4aa1-8997-e0dd994faedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "grep \"pragma omp\" reduction_scalar.c\n",
    "grep \"pragma omp\" reduction_array.c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05b444-acd8-4c7a-aed1-d2c4ee232c8d",
   "metadata": {},
   "source": [
    "Note that now we must include the dimensions of the output array in the `reduction` clause, to provide the compiler with the information it needs to effectively offload the variables correctly.\n",
    "In higher dimension cases, each further dimension is represented by an additional set of square brackets and co-ordinates.\n",
    "\n",
    "Let's compile and run this example now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d6c44-2760-4a29-9389-3e306f239ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "make reduction_array\n",
    "./reduction_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5495aa2-a868-406d-a94b-5db4ad342274",
   "metadata": {},
   "source": [
    "## Atomics\n",
    "\n",
    "The modern equivalent of a mutex for single assignments, the `omp atomic` directive ensures that the next memory operation immediately following it cannot be accessed by multiple threads at the same time.\n",
    "For single memory operations, this lock is far less costly than the full software lock available in directives such as `omp critical`, whilst still avoiding problematic race conditions.\n",
    "\n",
    "`omp critical` directives still have their uses; they are, for example, able to lock entire regions of code.\n",
    "In general, however, these full software locks of data regions are too costly for efficient device computing.\n",
    "It is often worth the time to modify the code to remove such locks when porting code to GPU.\n",
    "\n",
    "Let's take a look at [`atomic.c`](./C/atomic.c) - in this example we perform an element-wise summation of an array using two methods; an atomic calculation within a parallel loop and a reduction.\n",
    "Both are timed, to compare performance.\n",
    "Note that we need to enable `HSA_XNACK` for this example, as we are requiring `unified_shared_memory` to let the OS handle our memory movements for us.\n",
    "\n",
    "Let's compile and run it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fcbf1b-c75b-46ca-91eb-7058331596a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "export HSA_XNACK=1\n",
    "make atomic\n",
    "./atomic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534ccbe7-4d14-4375-b859-9ad55d22007c",
   "metadata": {},
   "source": [
    "As we can see, the reduction is several orders of magnitude faster than the atomic calculation.\n",
    "When writing performant code, it is therefore recommended to write it as a reduction wherever possible.\n",
    "Atomics can be very useful, but should be used sparingly, and only where strictly needed.\n",
    "\n",
    "In this notebook, we have learned about the concept of mutexes, and how modern algorithms in the form of reductions and atomic calculations have made them redundant for OpenMP code.\n",
    "In the next notebook, we will be looking at function calls in target regions in OpenMP, and bringing together the things we have learned in the exercises."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NI200",
   "language": "bash",
   "name": "ni200"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
