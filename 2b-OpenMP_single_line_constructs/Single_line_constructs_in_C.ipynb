{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8bfedd3-4096-4a90-b5a0-16557bf07059",
   "metadata": {},
   "source": [
    "# OpenMP Single Line Compute Constructs\n",
    "\n",
    "In this notebook, we will look at the use of single line directives as a means of moving the computation of a loop to the GPU.\n",
    "For the examples in this notebook, we will use a simple saxpy code, which we will discuss below.\n",
    "\n",
    "Let's begin, as ever, by checking that we have suitable GPUs available, moving into the source directory, and making sure that the working environment is clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee9694-fa7b-402a-b6a4-530ea5ba2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rocm-smi\n",
    "cd $HOME/DiRAC-AMD-GPU/notebooks/02-OpenMP/2b-OpenMP_single_line_constructs/C\n",
    "make clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5885839-91b4-4a3a-80b7-aee1314d3924",
   "metadata": {},
   "source": [
    "## The saxpy code\n",
    "\n",
    "Saxpy is an acronym that stands for **Single precision A X plus Y**.\n",
    "It refers to the addition of two vectors, X and Y, where X is multiplied by a scalar A.\n",
    "Practically, this is done by looping over the elements of X and Y and calculating each result individually.\n",
    "Mathematically, it can be expressed as;\n",
    "$$z_i = a \\cdot x_i + y_i$$\n",
    "for the $i^{th}$ element of the arrays.\n",
    "\n",
    "These sorts of simple vector additions are used extensively in simulation and graphical processing, and can be written into code as a loop over the elements of the two arrays.\n",
    "Since each calculation has no dependence on any other element in the array, they could, in principle, all be carried out independently.\n",
    "This makes saxpy a prime candidate for parallelisation across CPUs and GPUs alike.\n",
    "Indeed, it is often colloquially referred to as the 'Hello, World!' of GPU programming, as the first real code example you are likely to write and run in any new GPU framework.\n",
    "\n",
    "Let's look now at a traditional CPU implementation of the saxpy code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c1f56-0648-4317-a929-8971ca0f3bfe",
   "metadata": {},
   "source": [
    "## CPU saxpy code\n",
    "\n",
    "[`saxpy_cpu.c`](./C/saxpy_cpu.c) contains a basic implementation of saxpy code for a CPU.\n",
    "In the `main` block, we can see the instantiation and initialisation of the two arrays that we will be combining, `x` and `y`.\n",
    "The `saxpy` function then carries out the vector addition itself within the loop:\n",
    "```c\n",
    "for (int i = 0; i < N; i++) {\n",
    "   y[i] = y[i] + a * x[i];\n",
    "}\n",
    "```\n",
    "Note that in this example, and others that you might see, rather than returning a new vector $z$, we are doing the addition directly into `y`.\n",
    "\n",
    "You should notice that we already have an OpenMP pragma in this code:\n",
    "```c\n",
    "#pragma omp parallel for\n",
    "```\n",
    "This instructs the compiler to allow threaded parallelism of the loop on the CPU.\n",
    "\n",
    "For our example, we initialise the `y` array to `2`, the `x` array to `1` and the scalar `a` to `2`, so we expect every element of the output array `y` after the computation to contain the value `4`.\n",
    "\n",
    "We can build this code using the `saxpy_cpu` option from the [`Makefile`](./C/Makefile).\n",
    "Let's build and run it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cb0557-4d60-4a1d-bb12-4e6f46340ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "make saxpy_cpu\n",
    "./saxpy_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715e933b-71e1-4e0b-b9bf-92f424cc78d0",
   "metadata": {},
   "source": [
    "The code has run successfully with the expected output.\n",
    "\n",
    "Now, let's start changing the pragma instructions to offload our loop onto the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf813e6-9101-4736-bb49-51249ea92c9a",
   "metadata": {},
   "source": [
    "## Offloading arrays to the GPU\n",
    "\n",
    "We will now go about moving our calculation to the GPU, starting with a simplified version of the code.\n",
    "\n",
    "### Automatically and dynamically assigned arrays\n",
    "\n",
    "The code in [`saxpy_gpu_singleunit_autoalloc.c`](./C/saxpy_gpu_singleunit_autoalloc.c) contains a slightly altered version of the saxpy code, such that vector addition is done not in its own function but in the `main` loop itself.\n",
    "This allows the compiler more direct information on variables used in the loop, allowing it to better understand when and how it can offload information the the target device. \n",
    "This is taken a step further in this code by assigning `x` and `y` on the stack - the compiler is now fully aware of the array sizes that need to be moved in the following loop.\n",
    "Note that we've had to reduce the number of entries in the arrays in comparison to the previous example, to make sure we don't run out of memory on the stack.\n",
    "\n",
    "We can see that we've also now changed the OpenMP pragma instruction to:\n",
    "```c\n",
    "#pragma omp target teams distribute parallel for\n",
    "```\n",
    "\n",
    "Let's remind ourselves of what the directives in this pragma are doing:\n",
    " - `target` tells the compiler to transfer control and data from the host (CPU) to the device (GPU),\n",
    " - `teams` creates a series of thread teams that can execute the code in parallel,\n",
    " - `distribute parallel for` instructs the compiler to run the subsequent `for` loop in parallel, distributing it between the previously created thread teams.\n",
    "\n",
    "Put all together, this commonly-used construct asks that the subsequent `for` loop block be run in parallel on the GPU.\n",
    "Let's try building and running this code now, using the `saxpy_gpu_singleunit_autoalloc` option of the [`Makefile`](./C/Makefile).\n",
    "\n",
    "Remember that we can set the `LIBOMPTARGET_INFO` environment variable to report at runtime the data arguments passed to an OpenMP device kernel.\n",
    "In this way, we can ensure that we are correctly running on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed13034e-3459-41df-9b98-1a8a2a133705",
   "metadata": {},
   "outputs": [],
   "source": [
    "export LIBOMPTARGET_INFO=1\n",
    "make saxpy_gpu_singleunit_autoalloc\n",
    "./saxpy_gpu_singleunit_autoalloc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65736ec4-4215-4ec9-bb52-57a3aba9eeb2",
   "metadata": {},
   "source": [
    "Success!\n",
    "We've offloaded our saxpy loop to the GPU, and run the addition there in parallel.\n",
    "\n",
    "\n",
    "But what if we want to dynamically assign the size of the input arrays, rather than hard-code their sizes at the beginning of the code?\n",
    "In the example code [`saxpy_gpu_singleunit_dynamic.c`](./C/saxpy_gpu_singleunit_dynamic.c), we've changed their assignment to `malloc` commands whilst keeping the rest of the code the same.\n",
    "Surely this should run in the same way?\n",
    "\n",
    "\n",
    "This code can be built using the `saxpy_gpu_singleunit_dynamic` command, so let's build and run it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d7dff2-4d69-4985-8c32-5a21f06f104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "make saxpy_gpu_singleunit_dynamic\n",
    "./saxpy_gpu_singleunit_dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0538a873-a145-457f-ad15-0ab8a9421497",
   "metadata": {},
   "source": [
    "Oh no, something's gone wrong!\n",
    "From the error message, we can see that the GPU is failing to access the data of the array.\n",
    "In this case, the compiler doesn't have enough information to properly offload the dynamically assigned arrays - without strictly knowing their size, it cannot work out how much memory to assign on the GPU.\n",
    "Fortunately, for the GPUs we are working on - the MI200 series available on Cosma - we can solve this problem using the `HSA_XNACK` environment variable that we have discussed previously.\n",
    "Setting this to 1 allows the operating system to automatically assign and move memory between the host and device.\n",
    "\n",
    "Let's try running that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8614bc76-d130-4e02-a2b3-d2c118833762",
   "metadata": {},
   "outputs": [],
   "source": [
    "export HSA_XNACK=1\n",
    "./saxpy_gpu_singleunit_dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658d27aa-1182-4831-b917-c3e7c2d575c4",
   "metadata": {},
   "source": [
    "Now we see the expected results.\n",
    "\n",
    "\n",
    "Of course, attempting to run this code on other GPUs might still have issues - you might need to use a `map` clause in the omp pragma to direct the memory manually.\n",
    "\n",
    "\n",
    "### Moving saxpy from the main\n",
    "\n",
    "\n",
    "Now that we can offload our loop successfully, even for dynamically assigned arrays, let's return to the original example code that moved the saxpy operation into a separate function.\n",
    "[`saxpy_gpu_parallelfor.c`](./C/saxpy_gpu_parallelfor.c) shows how we can apply the new pragma to this code, in a separate function instead of in the `main` block.\n",
    "\n",
    "\n",
    "Let's compile and run this code, using the `saxpy_gpu_parallelfor` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa39e53-6e17-45e9-8eb3-b08c9653a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "make saxpy_gpu_parallelfor\n",
    "./saxpy_gpu_parallelfor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72441362-10df-483c-a1c7-c8ffcea1e3be",
   "metadata": {},
   "source": [
    "This works as expected.\n",
    "Note that in this example we still have `HSA_XNACK` set to 1 from the cell in the notebook.\n",
    "This must be set for this code to run; we will see the same error as before if we were to turn this setting back off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8f4360-dadf-4b6c-86d6-2e7d313db199",
   "metadata": {},
   "source": [
    "## The **loop** clause\n",
    "\n",
    "In more recent versions of OpenMP, the `loop` clause has been added as a simpler replacement to the `distribute parallel for` clause.\n",
    "The clause that we would need to write for our offloaded loop would then become:\n",
    "```c\n",
    "#pragma omp target teams loop\n",
    "```\n",
    "\n",
    "This new clause simplifies our usual pragmas, and allows the compiler more freedom in its implementation of parallelism for the applicable loop.\n",
    "Indeed, ROCm will generate an optimised target region using this pragma over the traditional `distribute parallel for` clause for AMD GPUs.  Additionally, from OpenMP version 6.0 it has changed replace the `teams distribute parallel for` clause. At the time of writing, not many compilers support OpenMP version 6.0, however, you should be aware of this upcoming change and check the documentation for your compiler of choice.\n",
    "\n",
    "This construct does currently have drawbacks, however.\n",
    "Because it is relatively novel, not all compilers will support its applications, and the additional freedom that it affords the compilers may not be optimised for all available cards.\n",
    "\n",
    "There is an example of the `loop` clause for our saxpy code in [`saxpy_gpu_loop.c`](./C/saxpy_gpu_loop.c).  Let's compile and run it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e3afe-4fd6-42c5-b567-b90927ed1a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "make saxpy_gpu_loop\n",
    "./saxpy_gpu_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1269e217-c481-41d0-8805-de0725de33b8",
   "metadata": {},
   "source": [
    "We see that the performance is largely consistent with the previous examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1800c06b-aafe-4eb4-ac1e-70a38302e772",
   "metadata": {},
   "source": [
    "## The **collapse** clause\n",
    "\n",
    "The final clause we will look at in this notebook is the `collapse(n)` clause.\n",
    "This clause requests that `n` nested loops be reduced (or 'collapsed') and executed as a single loop.\n",
    "\n",
    "We can see an example of this in the example code [`saxpy_gpu_collapse.c`](./C/saxpy_gpu_collapse.c).\n",
    "Here, we have transformed `x` and `y` into 2D arrays, and are performing the saxpy operation with nested loops over these dimensions.\n",
    "The `collapse` clause is then added to the end of our normal pragma to request the reduction to one dimension.\n",
    "\n",
    "In general, this use-case of the `collapse` clause is not recommended for use with C code, as we would usually prefer to transform the 2D array into a single 1D array whose index runs across all elements.\n",
    "It is, however, very common in Fortran codes that make extensive use of its native multi-dimensional arrays.\n",
    "\n",
    "Let's build and run the code, using the `saxpy_gpu_collapse` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40429abd-8051-4fea-b709-067eb9081e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "make saxpy_gpu_collapse\n",
    "./saxpy_gpu_collapse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cae26f-867b-4a9e-8ff7-52383eb12c19",
   "metadata": {},
   "source": [
    "As we've seen in this notebook, it is very easy to add a single line pragma that offloads and runs a loop on a GPU.\n",
    "Dynamic memory options available on the recent MI200 and MI300 series AMD GPUs make managing the memory involved in the process much simpler, so that we can focus on parallelising the appropriate loops rather than fiddling with what memory needs to go where.\n",
    "\n",
    "In the next section, we will look at some more complex constructs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NI200",
   "language": "bash",
   "name": "ni200"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
