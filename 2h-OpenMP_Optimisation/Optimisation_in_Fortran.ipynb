{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fc80049-50d7-44e4-a062-2ae628508f36",
   "metadata": {},
   "source": [
    "# OpenMP Optimisation \n",
    "\n",
    "So far in these notebooks, we have learnt about many of the pragmas available in OpenMP to offload calculations, memory, and even functions to GPU devices.\n",
    "Whilst judicious use of these pragmas should improve the performance of appropriate code, we have not yet spent any time considering performance and optimisations within OpenMP itself.\n",
    "This notebook will explore a few areas in which the performance of OpenMP itself can be tweaked, including; memory allocation optimisations; memory pools; and kernel tuning clauses.\n",
    "\n",
    "These examples are designed to work with unified shared memory or managed memory to keep them simpler.\n",
    "For this reason, we will set the `HSA_XNACK` environment variable to 1 now.\n",
    "Whilst here, let's do our standard environment check to make sure that we have an appropriate GPU to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce4f40-8559-49b3-923c-6e0d49a767a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "export HSA_XNACK=1\n",
    "rocm-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b87a03-0fe9-4e24-85ea-47c7418e6e3a",
   "metadata": {},
   "source": [
    "## Optimising memory allocations\n",
    "\n",
    "Although memory transfers are often the most costly part of offloading operations, there is also an overhead associated with memory allocations.\n",
    "Let's consider our first example, [`alloc_problem.f90`](./Fortran/1_alloc_problem/alloc_problem.f90).\n",
    "\n",
    "The main loop within this program calculates the value of $sin^2(i) + cos^2(i)$ for every member of a long array, and returns the sum of these divided by the number of elements - a value we expect to be 1.\n",
    "For demonstrative purposes, we are running this loop 10 times.\n",
    "Note that in this example, we carry out the memory allocations and deletion of the arrays within this loop.\n",
    "\n",
    "Let's compile this example, run it, and check the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb554f33-8865-4a6f-ab61-2e2708f71be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $HOME/DiRAC-AMD-GPU/notebooks/02-OpenMP/2h-OpenMP_Optimisation/Fortran/1_alloc_problem\n",
    "make clean\n",
    "make\n",
    "./alloc_problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7159ac2-fd4b-42dc-a651-4fb372e97676",
   "metadata": {},
   "source": [
    "The code has run successfully, and the final result has been reported correctly.\n",
    "\n",
    "Let's compare this with another version of the code, [`opt_allocation.f90`](./Fortran/2_opt_allocation/opt_allocation.f90).\n",
    "This code is identical, except that the memory allocations and deletions are carried out outside the main loop.\n",
    "Naively, we might not expect this to have a very large impact on the performance.\n",
    "\n",
    "Let's compile and run it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb508b5-0984-4119-a676-c9779db4a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $HOME/DiRAC-AMD-GPU/notebooks/02-OpenMP/2h-OpenMP_Optimisation/Fortran/2_opt_allocation\n",
    "make clean\n",
    "make\n",
    "./opt_allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7dabd-3d51-435e-9b52-b6ccf027528d",
   "metadata": {},
   "source": [
    "On the MI200X that we are running on, this optimisation of memory allocation has given us a better than 6x speed-up.  \n",
    "The speed-up is particularly dramatic on a discrete GPU such as this because, under the unified shared memory (USM) model, each new memory allocation on the host triggers page migrations when accessed from the device.\n",
    "These migrations are costly and can significantly impact performance.\n",
    "Without the USM model, we would still be required to add a `map` directive to the `target teams loop` clause, which would trigger explicit memory transfers for each iteration, that are also very costly.\n",
    "\n",
    "Where possible, memory allocations should be carried out once, outside of loops that use the variables multiple times.\n",
    "This allows the `target` directive to recognise the memory and avoids repeated page migrations by reusing already-migrated pages on the GPU.\n",
    "When not using USM, we could still allocate the host memory outside the loop and use an unmanaged memory region with a clause like `target enter data map(alloc:)` to ensure the memory allocation remains accessible to the device.\n",
    "This won't always be achievable - where arrays aren't necessarily of fixed size in every iteration, for example - but is good practice where it can be implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a691c4-cd57-4f96-9a7e-df7649987e3d",
   "metadata": {},
   "source": [
    "## Using memory pools\n",
    "\n",
    "Another approach to reducing the cost of memory allocations and deallocations is to use a memory pool.\n",
    "A memory pool pre-allocates an area of memory on the device that can be easily and efficiently allocated to necessary variables later on in the code.\n",
    "Many implementations of a memory pool are available, but for our example we will use the Umpire memory manager from Lawrence Livermore National Laboratory (LLNL), which works with C, C++ and Fortran.  \n",
    "\n",
    "This library is unlikely to be installed on the system you are running on, and so installing it will be the first step to using these memory pools.  \n",
    "The installation will take multiple steps and is likely similar to what you would encounter if you wished to install the library on another environment. \n",
    "1. ssh into the cosma login node if you are not already connected\n",
    "2. cd to `$HOME/DiRAC-AMD-GPU/notebooks/02-OpenMP/2h-OpenMP_Optimisation/Fortran/3_memorypool`\n",
    "3. Clone the library with the command `git clone --recursive https://github.com/LLNL/Umpire.git Umpire_source`\n",
    "4. Start a new terminal in the jupyter notebook.  This will create an interactive shell session on the AMD GPU node (note that this step is only needed as the software stack on the training node is different to the login node).\n",
    "5. In your Jupyter terminal, make sure you are in `$HOME/DiRAC-AMD-GPU/notebooks/02-OpenMP/2h-OpenMP_Optimisation/Fortran/3_memorypool` (you can check with `pwd`), and if not cd to it.\n",
    "6. Run the script `./umpire_setup` (note that this script is designed to run on the COSMA training node.  To install on the login node, you will have to change the variables and paths in the script accordingly). \n",
    "\n",
    "After installation, we need to set the environment variable to the installtion directory with `export UMPIRE_PATH=<path_to_install_dir>`.  This has already been done in the code block below.\n",
    "\n",
    "With the library installed, let's take a look at at our example code [`memorypool.f90`](./Fortran/3_memorypool/memorypool.f90).\n",
    "This code carries out the same calculations as our previous examples, but now with the inclusion of a memory pool.\n",
    "We create the shared pool before we enter the main calculation loop, then allocate and deallocate the arrays for our calculations in each iteration. \n",
    "\n",
    "Let's compile and run this code now, and compare its performance to our previous examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a8d57c-8d80-4528-94ab-1a7f9da99583",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $HOME/DiRAC-AMD-GPU/notebooks/02-OpenMP/2h-OpenMP_Optimisation/Fortran/3_memorypool\n",
    "export UMPIRE_PATH=\"./Umpire_install\"\n",
    "make clean\n",
    "make\n",
    "./memorypool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5dba46-2315-46ae-a0c5-7e2db6087865",
   "metadata": {},
   "source": [
    "The performance of the memory pool is comparable to the previous [`opt_allocation.f90`](./Fortran/2_opt_allocation/opt_allocation.f90) example.\n",
    "Memory pools offer the advantage of allowing dynamically sized arrays each iteration, without the usual associated cost of the memory allocation.\n",
    "\n",
    "Feel free to experiment with the different allocation methods, different size arrays and loops, and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819fcd1-c578-40d7-97e8-1a5ae336b595",
   "metadata": {},
   "source": [
    "## Kernel Tuning\n",
    "\n",
    "Now that we've seen how memory allocation can impact performance, let's take a look at some options to optimise the OpenMP pragmas themselves.\n",
    "First, we'll set `LIBOMPTARGET_KERNEL_TRACE=1` so we can better inspect what the OpenMP runtime is doing. \n",
    "The tracer allows us to see information about the number of teams and threads per team, the number of Scalar General Purpose Registers (SGPR) and Vector General Purpose Registers (VGPR).\n",
    "These are all effective proxies of how well we are utilising the GPU device.\n",
    "We'll also move into the `kernel_pragmas` directory containing our examples, and set up our build environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a302a565-f271-4970-8c6e-11b53252664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $HOME/DiRAC-AMD-GPU/notebooks/02-OpenMP/2h-OpenMP_Optimisation/Fortran/kernel_pragmas\n",
    "export LIBOMPTARGET_KERNEL_TRACE=1\n",
    "rm -rf build\n",
    "cmake -B build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a4115-ce5e-41c5-a501-472311560589",
   "metadata": {},
   "source": [
    "The baseline code for our tuning examples will be [`kernel1.f90`](./Fortran/kernel_pragmas/kernel1.f90).\n",
    "The code is a simple daxpy loop with the now familiar `target teams distribute parallel do` OpenMP clauses used for assignment and calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b436cac6-95b7-4a23-aea1-e7128b70ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "grep \"omp target teams distribute parallel\" kernel1.f90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cf794c-ad94-4da2-b592-c4b809586e3f",
   "metadata": {},
   "source": [
    "Lets build and run the code, taking note of the information printed out by the trace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1ef41b-13a0-4224-b4d5-1f33361e8bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmake --build build --target kernel1\n",
    "./build/kernel1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb565a8-c098-4de3-abf1-664f03284817",
   "metadata": {},
   "source": [
    "Note that the `Timing in Seconds` line gives us the timing of the daxpy kernel itself - increasing the `NTIMERS` macro will run the kernel multiple times, and provide average figures of the timing.\n",
    "Keep this baseline performance in mind as we investigate tuning parameters.\n",
    "\n",
    "### The `num_threads` clause\n",
    "\n",
    "With no explicit instructions, the compiler will decide how many threads to assign each team on the device.\n",
    "From the trace output above, we can see that the default for the daxpy kernel on our MI200 is 256 such threads.\n",
    "We can, however, control this ourselves by adding the `num_threads(XX)` clause to our target pragma, where `XX` is the number of desired threads per team.\n",
    "Let's see how we've implemented this in our next example code, [`kernel2.f90`](./Fortran/kernel_pragmas/kernel2.f90):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394ae77-7e9c-4a3d-aa31-f291ef157ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grep \"omp target teams distribute parallel\" kernel2.f90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca52723-ab1a-4dd1-a2ee-298d17920449",
   "metadata": {},
   "source": [
    "In this example, we've set the number of threads per team to be equal to 64.\n",
    "Let's compile and run it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfdc2e9-d0c4-4895-9d17-32df73d1c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmake --build build --target kernel2\n",
    "./build/kernel2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d5ec02-ddad-447a-9501-5626f0eca896",
   "metadata": {},
   "source": [
    "We can see from the trace that we have reduced the number of threads per team, and have consequently increased the number of teams produced for the code.\n",
    "\n",
    "### The `thread_limit` clause\n",
    "\n",
    "We can limit the number of number of threads possible in a given pragma using the `thread_limit` clause.\n",
    "The compiler will be free to select any number of threads per team up to the number given in the clause.\n",
    "We have implemented this clause in the [`kernel3.f90`](./Fortran/kernel_pragmas/kernel3.f90) example - let's look at it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e2945-4122-4fe8-8710-3517604a6629",
   "metadata": {},
   "outputs": [],
   "source": [
    "grep \"omp target teams distribute parallel\" kernel3.f90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efe60d7-df0e-4b00-9016-1365606a1ac9",
   "metadata": {},
   "source": [
    "We have applied a `thread_limit` of 64 to our pragmas here.\n",
    "Let's compile and run it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a9fcc-e804-4a67-83c7-ca6ece00d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmake --build build --target kernel3\n",
    "./build/kernel3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7180c6-0c8a-404c-9f12-85abc55dfe48",
   "metadata": {},
   "source": [
    "We are running over a short loop in this example, so the differences in performance will be small.\n",
    "Furthermore, compiler optimisations mean that the performance and improvement we see with these tunings will vary greatly over time, and differ significantly from those shown in the slides.\n",
    "\n",
    "### The `num_teams` clause\n",
    "\n",
    "The final kernel tuning clause of note that we will discuss here is the `num_teams` clause.\n",
    "As you might expect from the name, it instructs the compiler to create a specific number of teams for the `target` operation.\n",
    "Although we don't have an example of its use here, we encourage you to test it out in the above examples to check how it impacts performance.\n",
    "\n",
    "Indeed, the best way to get a better feel for these kernel tuning clauses is to try them out on more complex code.\n",
    "Try expanding the loop or increasing the complexity of the calculations involved, and then test some different values for `num_threads`, `num_teams` and `thread_limit`.\n",
    "The goal is to lower register usage in order to improve the occupancy.\n",
    "You can refer to the slides to see the number of \"waves\" that can run on a compute unit with respect to the number of VGPRs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b4df06-ab5d-4832-ae7b-4ddb87209fac",
   "metadata": {},
   "source": [
    "This notebook taught us about some optimisations that can help with OpenMP performance.\n",
    "Sometimes, even these optimisations aren't enough for the most intensive parts of our code - for those, we might want to use one of the other programming models such as HIP to improve our performance.\n",
    "The next section will discuss the interoperability of OpenMP and HIP code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NI200",
   "language": "bash",
   "name": "ni200"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
