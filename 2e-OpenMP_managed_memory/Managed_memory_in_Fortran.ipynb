{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85c789b6-7369-42dc-ac90-a8e9fd985e83",
   "metadata": {},
   "source": [
    "# Managed Memory\n",
    "\n",
    "The previous notebook taught us about explicitly managing memory across the host and device, and the directives that OpenMP provides for doing so.\n",
    "More modern AMD Instinct series GPUs provide functionality that allow the operating system (OS) to automatically manage these data transfers itself.\n",
    "In this notebook, we will look at some examples of managed memory code, how we can make it backwards compatible with non-unified memory devices, and the benefits that the unified memory can offer.\n",
    "\n",
    "Before start, let's move in to the appropriate directory, and set up a clean working environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd16f61a-7918-4d35-83d8-250cad76b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $HOME/DiRAC-AMD-GPU/notebooks/02-OpenMP/2e-OpenMP_managed_memory/Fortran\n",
    "rm -rf build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bb47b0-25a7-4bdf-b368-02ad437e5dc1",
   "metadata": {},
   "source": [
    "Let's now set the necessary environment variables for the exercises.\n",
    "As we've previously seen, `LIBOMPTARGET_INFO` provides information on memory transfers to the device.\n",
    "`LIBOMPTARGET_KERNEL_TRACE=1` prints message whenever a kernel is launched on the device, along with the number of teams and threads, and its register usage.\n",
    "And as introduced in the previous notebook, `OMP_TARGET_OFFLOAD=MANDATORY` will ensure that the code only runs if it is successfully offloading to its target device.\n",
    "In this notebook, we will again be using the `HSA_XNACK` environment variable, to tell the compiler to let the OS handle memory management.\n",
    "\n",
    "Let's enable these, and set up our build folder now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b9a7cd-e037-4005-870b-70d7980c5f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "export LIBOMPTARGET_KERNEL_TRACE=1\n",
    "export LIBOMPTARGET_INFO=1\n",
    "export OMP_TARGET_OFFLOAD=MANDATORY\n",
    "export HSA_XNACK=1\n",
    "cmake -B build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3a5f23-94c6-4a4c-8d89-a2887770fafe",
   "metadata": {},
   "source": [
    "## The example daxpy code with managed memory - `mem7.F90`\n",
    "\n",
    "The examples in this notebook use the same daxpy code as the previous, explicit memory management notebook.\n",
    "They offload a simple daxpy loop to the target device, and report on the time spent in the calculation.\n",
    "Let's look at the OpenMP pragmas that we use in the managed memory code, implemented in the first example [`mem7.F90`](./Fortran/mem7.F90), and compare them with the explicit model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0159c213-4775-42d7-a31d-7acb2c00c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grep \"\\!\\$omp\" mem7.F90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741addbe-2d76-49e7-b7bf-8fe960845d1c",
   "metadata": {},
   "source": [
    "We can see that there are two changes with respect to the baseline explicit managed memory case; the addition of a `requires unified_shared_memory` directive, and the removal of all `map` clauses.\n",
    "\n",
    "As you might expect from reading it, the `#!$omp requires unified_shared_memory` directive requires that the target architecture specified for offloading (i.e. the device specified using `--offload-arch`) supports unified shared memory.\n",
    "It is what's known as a declarative directive, and will cause the compilation to fail should the requirement not be met.\n",
    "The declaration of `unified_shared_memory` in this pragma makes the use of `map` in any subsequent `target device` directives optional, as it requires that the device is already explicitly able to see the memory address of any variables used within the target region.\n",
    "And indeed, we see that in this code the `map` clauses have been removed as unnecessary.\n",
    "\n",
    "Let's compile and run this code now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c07064-ca65-4d8f-924c-77ab55d88fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmake --build build --target mem7\n",
    "./build/mem7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707182b0-f12a-4472-ba36-26f97bdb3ef5",
   "metadata": {},
   "source": [
    "Note that in the output we see two reports from `LIBOMPTARGET_KERNEL_TRACE` when our target regions are launched, but no memory movement from `LIBOMPTARGET_INFO`.\n",
    "This is because in the unified shared memory model, no data transfer is taking place.\n",
    "\n",
    "## Maintaining backwards compatability for discrete GPUs - `mem8.F90`\n",
    "\n",
    "This code will work on any system with a unified shared memory, but will not be portable to any system that does not.\n",
    "If we want to keep the benefits of the unified model whilst also allowing it to run on other architectures, we will need to make some changes.\n",
    "[`mem8.F90`](./Fortran/mem8.F90) demostrates how we can implement this backwards-compatability into our code.\n",
    "Let's take a look at how it compares with the baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc45b0e-728d-4222-a7ae-a79bc88b12c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff mem8.F90 mem7.F90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac9558-0a20-4256-ade2-a34815399eca",
   "metadata": {},
   "source": [
    "The first change is adding in a pre-processor option around the `requires unified_shared_memory` directive.\n",
    "If we are compiling for an architecture that has shared unified memory, the pragma gets called as normal, but for an unsupported architecture it will now be ignored.\n",
    "This means that we'll now need to explicitly tell the compiler how to handle the data for such cases, and we can see this in the unstructured data region implemented for the variables `x`, `y` and `z` above.\n",
    "Remember that the requirement of `unified_shared_memory` only makes such implementations optional in the code; we are free to add memory directives as we wish, but with the unified model enabled the compiler may chose to ignore them where they are unnecessary.\n",
    "\n",
    "Let's compile and run this example now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ed16f-01a5-4d71-82b7-d937f9b9f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmake --build build --target mem8\n",
    "./build/mem8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d498550-3afd-430a-9b81-39ed47ef6b1b",
   "metadata": {},
   "source": [
    "We see the same output as the previous example - i.e. none of the `target data` directives have explicitly moved any data.\n",
    "This is to be expected on a system that supports shared unified memory.\n",
    "\n",
    "In this and the previous notebook, we have discussed how to manage memory - both explicitly and automatically by the OS - for large arrays on which we want to act on each element separately.\n",
    "This is the foundation of accelerated computation, but is hardly the end of the story.\n",
    "\n",
    "The next section will discuss how OpenMP treats distributed calculations that need to access a single universal variables, on the topic of reductions, atomics and mutexes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NI200",
   "language": "bash",
   "name": "ni200"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
