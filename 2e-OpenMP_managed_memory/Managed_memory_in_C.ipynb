{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22eb95f-fb17-4140-9d4d-78545c01af08",
   "metadata": {},
   "source": [
    "# Managed Memory\n",
    "\n",
    "The previous notebook taught us about explicitly managing memory across the host and device, and the directives that OpenMP provides for doing so.\n",
    "More modern AMD Instinct series GPUs provide functionality that allow the operating system (OS) to automatically manage these data transfers itself.\n",
    "In this notebook, we will look at some examples of managed memory code, how we can make it backwards compatible with non-unified memory devices, and the benefits that the unified memory can offer.\n",
    "\n",
    "Before start, let's move in to the appropriate directory, and set up a clean working environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd16f61a-7918-4d35-83d8-250cad76b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $HOME/DiRAC-AMD-GPU/02-OpenMP/2e-OpenMP_managed_memory/C\n",
    "rm -rf build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f391a7-7cb6-451b-8297-2f87fb0dd198",
   "metadata": {},
   "source": [
    "Let's now set the necessary environment variables for the exercises.\n",
    "As we've previously seen, `LIBOMPTARGET_INFO` provides information on memory transfers to the device.\n",
    "`LIBOMPTARGET_KERNEL_TRACE=1` prints message whenever a kernel is launched on the device, along with the number of teams and threads, and its register usage.\n",
    "And as introduced in the previous notebook, `OMP_TARGET_OFFLOAD=MANDATORY` will ensure that the code only runs if it is successfully offloading to its target device.\n",
    "In this notebook, we will again be using the `HSA_XNACK` environment variable, to tell the compiler to let the OS handle memory management.\n",
    "\n",
    "Let's enable these, and set up our build folder now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b9a7cd-e037-4005-870b-70d7980c5f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "export LIBOMPTARGET_KERNEL_TRACE=1\n",
    "export LIBOMPTARGET_INFO=1\n",
    "export OMP_TARGET_OFFLOAD=MANDATORY\n",
    "export HSA_XNACK=1\n",
    "cmake -B build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebad53f-e148-4a6d-b9e9-3b6ae32443ec",
   "metadata": {},
   "source": [
    "## The example daxpy code with managed memory - `mem7.cc`\n",
    "\n",
    "The examples in this notebook use the same daxpy code as the previous, explicit memory management notebook.\n",
    "They offload a simple daxpy loop to the target device, and report on the time spent in the calculation.\n",
    "Let's look at the OpenMP pragmas that we use in the managed memory code, implemented in the first example [`mem7.cc`](./C/mem7.cc), and compare them with the explicit model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9ae7c-60d2-4906-b6f3-a2821e335b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "grep \"pragma omp\" mem7.cc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58890be-e5d0-4f9e-8f30-65d64d22df8a",
   "metadata": {},
   "source": [
    "We can see that there are two changes with respect to the baseline explicit managed memory case; the addition of a `requires unified_shared_memory` directive, and the removal of all `map` clauses.\n",
    "\n",
    "As you might expect from reading it, the `#pragma omp requires unified_shared_memory` directive requires that the target architecture specified for offloading (i.e. the device specified using `--offload-arch`) supports unified shared memory.\n",
    "It is what's known as a declarative directive, and will cause the compilation to fail should the requirement not be met.\n",
    "The declaration of `unified_shared_memory` in this pragma makes the use of `map` in any subsequent `target device` directives optional, as it requires that the device is already explicitly able to see the memory address of any variables used within the target region.\n",
    "And indeed, we see that in this code the `map` clauses have been removed as unnecessary.\n",
    "\n",
    "Let's compile and run this code now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c07064-ca65-4d8f-924c-77ab55d88fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmake --build build --target mem7\n",
    "./build/mem7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33be3736-7d83-4af2-8161-1248fd0f7f1f",
   "metadata": {},
   "source": [
    "Note that in the output we see two reports from `LIBOMPTARGET_KERNEL_TRACE` when our target regions are launched, but no memory movement from `LIBOMPTARGET_INFO`.\n",
    "This is because in the unified shared memory model, no data transfer is taking place.\n",
    "\n",
    "## Maintaining backwards compatability for discrete GPUs - `mem8.cc`\n",
    "\n",
    "This code will work on any system with a unified shared memory, but will not be portable to any system that does not.\n",
    "If we want to keep the benefits of the unified model whilst also allowing it to run on other architectures, we will need to make some changes.\n",
    "[`mem8.cc`](./C/mem8.cc) demostrates how we can implement this backwards-compatability into our code.\n",
    "Let's take a look at how it compares with the baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495c18c4-e605-473f-95c8-e6891154293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff mem8.cc mem7.cc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7602f305-d5b6-4111-aaf0-16521dac8050",
   "metadata": {},
   "source": [
    "The first change is adding in a pre-processor option around the `requires unified_shared_memory` directive.\n",
    "If we are compiling for an architecture that has shared unified memory, the pragma gets called as normal, but for an unsupported architecture it will now be ignored.\n",
    "This means that we'll now need to explicitly tell the compiler how to handle the data for such cases, and we can see this in the unstructured data region implemented for the variables `x`, `y` and `z` above.\n",
    "Remember that the requirement of `unified_shared_memory` only makes such implementations optional in the code; we are free to add memory directives as we wish, but with the unified model enabled the compiler may chose to ignore them where they are unnecessary.\n",
    "\n",
    "Let's compile and run this example now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ed16f-01a5-4d71-82b7-d937f9b9f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmake --build build --target mem8\n",
    "./build/mem8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e774d7-55e3-4f4f-aae6-03e0eec36b04",
   "metadata": {},
   "source": [
    "We see the same output as the previous example - i.e. none of the `target data` directives have explicitly moved any data.\n",
    "This is to be expected on a system that supports shared unified memory.\n",
    "\n",
    "## Use of `std::vector` in offloaded variables - `mem9.cc`\n",
    "\n",
    "In all of our previous examples, we have been using C-style arrays to offload data to the device.\n",
    "The rigid size of an allocated array makes these variables well suited for the memory offloads associated with running on a target device, but can be detrimental in regular coding activities when the size of such an array is not known at compile-time, or changes during running.\n",
    "For such cases, it is often advantageous to use standard library containers such as `std::vector`, allowing dynamic reallocation of size and easy appending and removing of elements thanks to the container's in-built memory management.\n",
    "\n",
    "In the traditional model of explicit memory offloading to a discrete GPU, `std::vector`s present an impossible challenge; if the class reallocates memory to a shifted vector, the device memory map is no longer valid.\n",
    "But with a unified shared memory, the device can work on `std::vector`s as well as it could on arrays.\n",
    "\n",
    "[`mem9.cc`](./C/mem9.cc) demonstrates our example code with the arrays replaced with `std::vector`s.\n",
    "When we compile this example, we will see a number of warnings informing us that the memory mapping of the `std::vector` type to the device will not necessarily be correct.\n",
    "It is important to understand this warning, particularly when using discrete GPUs and allowing the OS to manage the memory movements for you, but in the unified memory space on an APU where no data is copied, the code will always work as expected.\n",
    "\n",
    "Let's compile and run it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9162a7-bde3-465a-ac84-d2966bb2f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmake --build build --target mem9\n",
    "./build/mem9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845caada-9307-4548-adb7-3b7b5ead6f9a",
   "metadata": {},
   "source": [
    "## Use of `std::valarray` in offloaded variables - `mem10.cc`\n",
    "\n",
    "In some legacy code, you may encounter the use of `std::valarray` as an alternative to the array, originally optimised for HPC use.\n",
    "With managed memory enabled, these may be used in OpenMP applications in the same way as the `std::vector`.\n",
    "[`mem10.cc`](./C/mem10.cc) shows our example code with `std::valarray`s replacing the array-typed variables.\n",
    "\n",
    "We will see the same warnings at compile time for this example as we did with `std::vector`s, and as in that case, we can expect it to run correctly if our system has a unified shared memory.\n",
    "Let's compile and run it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ad458-0e81-47c8-92d9-3f3a23571dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmake --build build --target mem10\n",
    "./build/mem10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4759269-56cb-4323-9fa4-8c007034d0ff",
   "metadata": {},
   "source": [
    "In this and the previous notebook, we have discussed how to manage memory - both explicitly and automatically by the OS - for large arrays on which we want to act on each element separately.\n",
    "This is the foundation of accelerated computation, but is hardly the end of the story.\n",
    "\n",
    "The next section will discuss how OpenMP treats distributed calculations that need to access a single universal variables, on the topic of reductions, atomics and mutexes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NI200",
   "language": "bash",
   "name": "ni200"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
