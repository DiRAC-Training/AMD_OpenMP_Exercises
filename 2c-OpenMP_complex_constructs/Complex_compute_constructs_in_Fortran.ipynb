{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f173f4d9-8e5c-4b5e-a233-01de9331b5c5",
   "metadata": {},
   "source": [
    "# OpenMP complex compute constructs\n",
    "\n",
    "In the previous section's exercises on simple single line compute constructs, we made extensive use of the combined compute directive `!$omp target teams distribute parallel do`.\n",
    "At the time, we discussed briefly what each clause in this directive was doing, and in this notebook we will explore what happens if we break apart this instruction and combine the clauses in different ways.\n",
    "We will then investigate how different approaches to parallelising nested loops impacts the performance of the code.\n",
    "\n",
    "Let us begin by checking that we have an appropriate GPU to run our code on using the `rocm-smi`, moving into our work directory and making sure that our work environment is clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75557c0b-f8ea-4bee-b3bd-e20c62868be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rocm-smi\n",
    "cd $HOME/DiRAC-AMD-GPU/notebooks/02-OpenMP/2c-OpenMP_complex_constructs/Fortran\n",
    "make clean\n",
    "export FC=/opt/rocm-6.3.0/bin/amdflang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71880ae5-52ed-4b48-9005-e4662efd20db",
   "metadata": {},
   "source": [
    "For these exercises, we want to see what the implementation is doing for each compute directive.\n",
    "We can do this by setting the `LIBOMPTARGET_KERNEL_TRACE` environment to 1.\n",
    "As the name suggests, this will print a trace for each OpenMP kernel encountered during runtime, and will give us information on the details of the execution.\n",
    "We will see this in action soon - for now, let's set those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac0925-da9b-409d-9fae-f0015ed27386",
   "metadata": {},
   "outputs": [],
   "source": [
    "export LIBOMPTARGET_KERNEL_TRACE=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f51d28c-c164-4f2c-b351-4b4bbd9ca4e9",
   "metadata": {},
   "source": [
    "## Breaking down compute constructs\n",
    "\n",
    "Firstly, let's return to the full directive.\n",
    "The code in [`saxpy_gpu_paralleldo.F90`](./Fortran/saxpy_gpu_paralleldo.F90) is the same as that in the previous notebook - that is to say, a simple saxpy code offloaded to the GPU using the `!$omp target teams distribute parallel for` directive.\n",
    "\n",
    "Let's build and run it, and then we can break down the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56bbdd-43fa-4ad2-b66e-64d97c023b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "make saxpy_gpu_paralleldo\n",
    "./saxpy_gpu_paralleldo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f47af46-3f80-4b3f-bc08-345d6f53031e",
   "metadata": {},
   "source": [
    "There is a lot of information here.\n",
    "Let's break down the important outputs here:\n",
    " - **DEVID** - the ID of the device  the kernel is running on,\n",
    " - **teamsXthrds** - the number of teams generated, and how many threads are within each team,\n",
    " - **sgpr_count** - the number of scalar registers required by a wavefront,\n",
    " - **vgpr_count** - the number of vector registers required by each work-item.\n",
    "\n",
    "For the full combined directive, we see that 624 teams are generated with 256 threads each.\n",
    "There is a vector register usage of 108.\n",
    "Bear in mind also the time this kernel took to run - which should be of order 0.025 seconds - as we will use this as a comparison metric as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f82e8e-0536-4dcd-aa99-7fdaf7e7044e",
   "metadata": {},
   "source": [
    "### The target directive\n",
    "\n",
    "Let's start by looking at the `target` directive itself.\n",
    "In the example code [`saxpy_gpu_target.F90`](./Fortran/saxpy_gpu_target.F90), we use the target directive with no additional clauses.\n",
    "Let's see this directive as it is in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7229a508-b8f5-48fa-b808-83ffae7af654",
   "metadata": {},
   "outputs": [],
   "source": [
    "grep \"omp\" saxpy_gpu_target.F90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c62b1c-c7ae-4b10-bc88-07b9d8b0521c",
   "metadata": {},
   "source": [
    "The target directive instructs the compiler to offload the enclosed code to the device.\n",
    "In our example, this is the subsequent saxpy `do` loop.\n",
    "With no further instructions or tips given to the compiler on what the code might be doing or how we might want it to run, let's find out how well it can handle the loop for itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b0d1d6-5fe8-4606-9670-c241d5785871",
   "metadata": {},
   "outputs": [],
   "source": [
    "make saxpy_gpu_target\n",
    "./saxpy_gpu_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea93bf93-429b-4b8b-b531-7191ea0ad979",
   "metadata": {},
   "source": [
    "Whilst we have successfully offloaded the code the target device, we can see that we have only created a single working group of 256 threads.\n",
    "In this configuration, there is little point to using the GPU at all, a fact that is borne out by a run time almost two orders of magnitude larger than our original baseline run.\n",
    "We are effectively running the code in serial.\n",
    "\n",
    "But of course without adding in any parallelisation instructions to the `target` directive, we should have expected this.\n",
    "Let's look at what happens if we now spawn a number of parallel processes, but give them no further directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92c2ffd-e092-44b6-8c63-2bd891d3a1be",
   "metadata": {},
   "source": [
    "### The teams clause\n",
    "\n",
    "In the example code [`saxpy_gpu_target_teams.F90`](./Fortran/saxpy_gpu_target_teams.F90), we have added the `teams` clause to the `target` directive.\n",
    "Let's see the current pragma now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3712457-9940-4933-8e79-bdf30db15110",
   "metadata": {},
   "outputs": [],
   "source": [
    "grep \"omp\" saxpy_gpu_target_teams.F90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a64b4-9aae-40f0-b360-cdd3e3fbb42c",
   "metadata": {},
   "source": [
    "The `teams` clause instructs the pragma to create a \"league of teams\".\n",
    "The initial thread of each team executes the code region with all of the data on each thread.\n",
    "Let's see how the target teams instruction handles our loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4919b674-7063-43f1-bd90-eac1d305668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "make saxpy_gpu_target_teams\n",
    "./saxpy_gpu_target_teams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755cf593-0737-4727-85db-df7645e48e66",
   "metadata": {},
   "source": [
    "We've successfully offloaded the code, and created a set of 624 teams to run the code, but we've still ended up with a significantly longer run time.\n",
    "So what is going on here?\n",
    "\n",
    "The answer is that although we've created a set of teams to run our loop, and they are indeed doing so in parallel.\n",
    "But without any instructions that the loop can be split up in any way, each team ends up running the entire loop itself!\n",
    "\n",
    "You may also notice that the results we get out from the loop are not the expected values of `y[0] 4.000000` and `y[N-1] 4.000000`.\n",
    "This is because each team is running using the same arrays, with no atomic protection.\n",
    "This leads to race conditions and array elements being added and written in ways in which they were not intended.\n",
    "\n",
    "Let's look now at letting the teams share the work properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69fc2d2-0aac-451f-9d36-3f6f8aa3eed0",
   "metadata": {},
   "source": [
    "### The distribute clause\n",
    "\n",
    "The code [`saxpy_gpu_target_teams_distribute.F90`](./Fortran/saxpy_gpu_target_teams_distribute.F90) adds the `distribute` clause to the `target` directive. The pragma now looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab9400-edc1-4793-b52e-2366d63f6d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "grep \"omp\" saxpy_gpu_target_teams_distribute.F90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c7a9c7-1be8-47c7-829e-2eb61fa33602",
   "metadata": {},
   "source": [
    "The `distribute` clause splits up loop iterations across available teams, and executes them on the main thread.\n",
    "This should solve both problems we had in the previous example by properly atomising the running of the code.\n",
    "Let's see it in action now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4013dd-578e-4500-b200-f03fe73e80d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "make saxpy_gpu_target_teams_distribute\n",
    "./saxpy_gpu_target_teams_distribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb37f18-c82f-4684-a891-93772e13f3d2",
   "metadata": {},
   "source": [
    "Success!\n",
    "Even without the `parallel do` clause, we are now approaching the same speed as our baseline with the full directive.\n",
    "\n",
    "Try changing the array size in the example, or replacing the kernel with something with more work, and see how the run time and load changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52db8c84-ee44-4a20-bd22-0e86e2396274",
   "metadata": {},
   "source": [
    "### Parallel for without a teams distribute clause\n",
    "\n",
    "As a final experiment, let's see what happens if we include the `parallel do` clause in our `target` directive without the `teams distribute` clauses.\n",
    "The example [`saxpy_gpu_target_parallel_do.F90`](./Fortran/saxpy_gpu_target_parallel_do.F90) has this already set up for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9a960e-cae1-49cb-8b27-295c7d59f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "make saxpy_gpu_target_parallel_do\n",
    "./saxpy_gpu_target_parallel_do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6492b30-d460-44bb-978e-97447e53563f",
   "metadata": {},
   "source": [
    "As expected, no additional teams have been created, but we still see a run time approaching that of the original baseline case.\n",
    "The `parallel` clause creates multiple threads within a team, and the `for` clause spreads out the work between these threads.\n",
    "So even without a high teams count, the `parallel for` clause can give very good performance.\n",
    "This can be used to run efficiently with fewer GPU compute units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56deb48f-e425-4655-9c28-9416dd81e85d",
   "metadata": {},
   "source": [
    "## Exploring the split level directives\n",
    "\n",
    "Finally in this notebook, let's look at different approaches to offloading nested `do` loops to the GPU.\n",
    "\n",
    "[`saxpy_gpu_collapse.F90`](./Fortran/saxpy_gpu_collapse.F90) contains a nested `do` loop parallelised using the `collapse` directive discussed in the previous notebook.\n",
    "The code of interest is the loop itself:\n",
    "\n",
    "```Fortran\n",
    "!$omp target teams distribute parallel do collapse(2)\n",
    "do j=1,n\n",
    "  do i=1,m\n",
    "    y(i,j) = y(i,j) + a * x(i,j)\n",
    "  end do\n",
    "end do\n",
    "```\n",
    "\n",
    "As we previously discussed, the `collapse(2)` clause asks the compiler to collapse the 2 subsequent loops into a single one that can be distributed out amongst the GPU computer.\n",
    "\n",
    "But now that we've tested the various clauses of this directive, can we not think of another way of splitting this code on the GPU?\n",
    "Could we not envisage each iteration of the first loop running its own second loop independently, hopefully also in parallel?\n",
    "\n",
    "The example code [`saxpy_gpu_split_level.F90`](./Fortran/saxpy_gpu_split_level.F90) does exactly that in its revised version of the nest `do` loops:\n",
    "\n",
    "```Fortran\n",
    "!$omp target teams distribute   \n",
    "do j=1,n\n",
    "  !$omp parallel do             \n",
    "  do i=1,m\n",
    "    y(i,j) = y(i,j) + a * x(i,j)\n",
    "  end do\n",
    "end do                                   \n",
    "```\n",
    "\n",
    "Here we create a set of teams and distribute the first loop amongst them.\n",
    "Then the second loop is parallelised within each team.\n",
    "\n",
    "Which approach do you think will be faster?\n",
    "\n",
    "Let's run them both now and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18890f37-c7fc-4e9b-a4d5-1997685bc232",
   "metadata": {},
   "outputs": [],
   "source": [
    "make saxpy_gpu_collapse\n",
    "./saxpy_gpu_collapse\n",
    "make saxpy_gpu_split_level\n",
    "./saxpy_gpu_split_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7799f456-dfbe-453f-a0de-bcdad2cbe852",
   "metadata": {},
   "source": [
    "There is almost nothing to chose between the two in terms of run time, at least in their original configuration.\n",
    "\n",
    "Try changing the size of the input arrays, and seeing how this impacts performance.\n",
    "\n",
    "You should notice that at larger sizes (say `N=10000,M=10000`), the collapse directive begins running much faster than the split level directive case.\n",
    "This is particularly interesting in comparison with the C version of this code, in which the split directive runs faster at larger array sizes.\n",
    "Indeed, the Fortran collapse statement runs substantially faster than any alternative in C or Fortran at these high array sizes, demonstrating the suitability of Fortran to large multi-dimensional array calculations on GPU.\n",
    "\n",
    "Now that we've explored the `target` directive and some of its many possible clauses and configurations, let's move on to memory management directives.\n",
    "In the next section, we will at explicit memory directive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NI200",
   "language": "bash",
   "name": "ni200"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
