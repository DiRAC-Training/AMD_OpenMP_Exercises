{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92332299",
   "metadata": {},
   "source": [
    "# OpenMP and HIP Interoperability\n",
    "\n",
    "In this notebook, we will see how we can write an application that makes use of both OpenMP pragmas and HIP kernels.\n",
    "This type of strategic use of multiple acceleration paradigms can make our code flexible and performant in ways that would not be possible with just one.\n",
    "\n",
    "Let's begin in the traditional way; by checking that we have an appropraite GPU on the system, loading into the relevant working directory and cleaning our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39f842-73d2-4ff0-8d1f-9ba4b0b9790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rocm-smi\n",
    "cd $HOME/DiRAC-AMD-GPU/notebooks/02-OpenMP/2i-OpenMP_HIP_interportability/C\n",
    "make clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99e5bb",
   "metadata": {},
   "source": [
    "## Why use HIP and OpenMP?\n",
    "\n",
    "As we've learned throughout these notebooks, OpenMP is a flexible and portable pragma-based API for shared-memory parallelism.\n",
    "It allows relatively straightforward parallelism to be implemented across different architectures, but lacks the level of direct control that other paradigms offer.\n",
    "\n",
    "HIP, on the other hand, offers a tighter control at the cost of OpenMP's ease and flexibility.\n",
    "The details of HIP implementation will not be discussed in this notebook - rather, that will be saved for the next section of this course.\n",
    "For now, it is enough to know that it is a low-level language that makes use of kernels to offer a more direct control of the offloaded compute.\n",
    "\n",
    "Both of these approaches have their upsides and their drawbacks, so you might wonder if there was a way to incorporate both into code, to draw on their relative strengths.\n",
    "And you would indeed be correct - there is!\n",
    "Code making use of OpenMP pragmas can call HIP kernels, and HIP applications can call OpenMP kernels.\n",
    "\n",
    "Using this knowledge we can, for example, write our simple loops using OpenMP pragmas, but write our complicated offloaded routines where we require fine-grained control to optimise their performance as HIP kernels.\n",
    "By using both in the same application, we really can have the best of both worlds.\n",
    "\n",
    "### The example code\n",
    "\n",
    "An example of this can be seen in the [`daxpy.cc`](./C/daxpy.cc) example.\n",
    "In this code, we define two arrays `x` and `y` and move them to the device in a structured data region.\n",
    "Several operations are carried out on the host, and the copies of `x` and `y` are necessarily updated on the device.\n",
    "Note that with the use of an APU, these memory pragmas would be unnecessary.\n",
    "\n",
    "We then carry out a daxpy loop using these two arrays, but this time in a HIP kernel.\n",
    "The HIP kernel itself is defined within the `daxpy_hip` function.\n",
    "This code seems relatively straighforward, so let's take a look at how we should go about compiling it.\n",
    "\n",
    "## Compiling code with HIP and OpenMP\n",
    "\n",
    "Unfortunately, whilst OpenMP applications can call HIP kernels, and vice versa, there is currently no support for compiling HIP and OpenMP at the same time.\n",
    "The HIP compiler `hipcc` cannot offload OpenMP GPU kernels, and the AMD compilers that support OpenMP pragmas cannot compile HIP kernels.\n",
    "In order to use both, we will need to compiler the HIP kernels using `hipcc`, and the OpenMP code with a suitable C++ compiler, such as `amdclang`.\n",
    "\n",
    "This can be done by separating the kernels into different files, and compiling them individually.\n",
    "As long as the kernels are properly declared in each other's scopes at compile-time, they will be able to call each other during runtime without issue.\n",
    "\n",
    "It is also possible, however, to have both in the same file and copmile in one command, via `make` and the prudent use of pre-processor flags.\n",
    "This concept is demonstrated in [`daxpy.cc`](./C/daxpy.cc).\n",
    "\n",
    "Note the two pre-processor commands that surround the majority of the code in this file, `#ifdef __DEVICE_CODE__` and `#ifdef __HOST_CODE__`.\n",
    "As you might expect from their names, the HIP kernels destined to run on the device are contained within the `__DEVICE_CODE__` pre-processor conditional statement, and the OpenMP code that will run on the host can be found within `__HOST_CODE__`.\n",
    "\n",
    "Now let's take a look at the [`Makefile`](./C/Makefile) and see how we can interact with these blocks of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat Makefile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1c8d63",
   "metadata": {},
   "source": [
    "Our default build rule builds our file into two objects; the first uses the `hipcc` compiler and second the default `CXX`.\n",
    "In the `HIPCC_FLAGS`, note the `-D__DEVICE_CODE` option.\n",
    "This defines the `__DEVICE_CODE__` macro and makes it available to the compiler, allowing it to enter the section of code in [`daxpy.cc`](./C/daxpy.cc) demarcated by the `__DEVICE_CODE__` `#ifdef` and compile it with the HIP libraries.\n",
    "\n",
    "We then compile the file again with the default `CXX` compiler, with the `-D__HOST_CODE__` flag.\n",
    "This gates the HIP section of code and compiles the OpenMP commands as usual.  We are then free to link these objects into the `daxpy` executable.  \n",
    "\n",
    "Let's try compiling and running it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff58fd-adff-44c6-9677-98632dfef243",
   "metadata": {},
   "outputs": [],
   "source": [
    "make daxpy\n",
    "./daxpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6986f66c",
   "metadata": {},
   "source": [
    "If all has worked well, the code will state which machine it was compiled for - the host or the device - and verify that the results were reported correctly.\n",
    "Congratulations!\n",
    "You've now successfully run code with multiple forms of acceleration enabled.\n",
    "\n",
    "This concludes the tutorial's optional lessons on OpenMP.\n",
    "\n",
    "In the next set of notebooks, we will dive in to HIP proper; examining its functionality; discovering the strengths of its programming model; and learning how we can implement it into our code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NI200",
   "language": "bash",
   "name": "ni200"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
